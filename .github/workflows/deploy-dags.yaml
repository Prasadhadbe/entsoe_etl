name: Deploy Airflow DAGs to Azure File Share

on:
  push:
    branches:
      - main
    paths:
      - "dags/**"

env:
  ACR_NAME: airflowacrcluster
  AKS_RESOURCE_GROUP: airflow-rg
  AKS_CLUSTER_NAME: airflow-aks
  IMAGE_NAME: airflow
  IMAGE_TAG: latest

jobs:
  deploy:
    name: Upload DAGs to Azure File Share
    runs-on: ubuntu-latest
    environment: etl-workflow

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Log in to Azure
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Upload DAGs (Fixed)
        run: |
          # Generate 2-hour SAS token
          EXPIRY=$(date -u -d "2 hours" '+%Y-%m-%dT%H:%MZ')
          SAS=$(az storage account generate-sas \
            --account-name f314a19b0d8b1437a879f40 \
            --account-key "${{ secrets.AZURE_STORAGE_KEY }}" \
            --services f \
            --resource-types sco \
            --permissions cdlruwap \
            --expiry $EXPIRY \
            --output tsv)

          # Upload with SAS token
          az storage file upload-batch \
            --destination "https://f314a19b0d8b1437a879f40.file.core.windows.net/pvc-a97254bb-1499-4e7e-bc74-cd016e8fa2bf/dags?$SAS" \
            --source "./dags" \
            --max-connections 8 \
            --verbose

      - name: Restart Airflow Deployments
        run: |
          echo "ðŸ”„ Restarting Airflow pods to reload DAGs..."
          az aks get-credentials --resource-group $AKS_RESOURCE_GROUP --name $AKS_CLUSTER_NAME --overwrite-existing
          kubectl rollout restart deployment airflow-webserver -n airflow
          kubectl rollout restart deployment airflow-scheduler -n airflow
          echo "âœ… Restart complete!"
